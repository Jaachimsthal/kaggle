{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9295fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 10:07:34,768 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 10:07:35.671910: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-30 10:07:36.269869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-30 10:07:38.097978: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-30 10:07:41,456 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n",
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 10:07:42,460 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(151936, 896)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入AutoModel类，该类允许自动从预训练模型库加载模型\n",
    "from modelscope import AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 设置预训练模型的检查点名称，这里使用THUDM维护的ChatGLM3-6B模型\n",
    "check_point = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# 将可选的本地模型路径注释掉，如果需要从本地加载模型，则取消注释并指定正确的本地路径\n",
    "# model_path = \"/home/egcs/models/chatglm3-6b\"\n",
    "# 使用AutoModel的from_pretrained方法加载模型表示信任远程代码，允许从模型仓库执行未验证的代码\n",
    "model: nn.Module = AutoModel.from_pretrained(pretrained_model_name_or_path=check_point, trust_remote_code=True, dtype=\"auto\").half().cuda()\n",
    "# 加载模型对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=check_point)\n",
    "# 加载模型对应的配置\n",
    "conf = AutoConfig.from_pretrained(pretrained_model_name_or_path=check_point)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34970a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(151936, 896)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.add_module(module=nn.Linear(896, 2), name=\"classification_layer\") # 兼容PyTorch操作\n",
    "model_from_conf = AutoModel.from_config(config=conf)\n",
    "model_from_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613a09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[108386,   3837, 105043, 100165,  11319, 151643],\n",
       "        [ 35946, 100165, 104993,   3837, 104198,  56568],\n",
       "        [102048,  12857,  44928, 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"“你好”\"))\n",
    "# 编码使用\n",
    "inputs = tokenizer([\"你好，你是谁？\", \"我谁也不是，我是你\", \"出口成章\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs_ids = inputs['input_ids'].to(\"cuda\")\n",
    "inputs_ids.shape\n",
    "inputs_ids\n",
    "# 解码使用\n",
    "tokenizer.decode(inputs_ids[2])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe1eca",
   "metadata": {},
   "source": [
    "# 检视模型（Qwen/Qwen2.5-0.5B-Instruct）\n",
    "\n",
    "## embed_tokens\n",
    "Embedding(151936, 896)，词嵌入层，输入维度为vocab_size=151936，转为896维向量\n",
    "## layers（ModuleList）\n",
    "一个标准的Decoder Only Transformers架构模型，这里Layer为TransformersBlocks；\n",
    "1. 多头自注意力层重复24次：\n",
    "    1. query：表示权重，维度为896*896，使用Linear层便于权重和input序列的矩阵乘法和权重参数的初始化；\n",
    "    2. key：维度为896*128，query @ key.T，query的行数=key.T的列数；\n",
    "    3. value：与key的维度相同，这里k和v的维度为128是因为当前为多头注意力，将128*7分为了7个头；\n",
    "    4. output：最后一个线性输出将输出维度重新设置为896*896；\n",
    "2. MLP（Qwen2MLP，多层感知机层）：是一个多层感知机，负责对经过自注意力机制处理后的向量表示进行非线性变换，以便捕捉更复杂的语义模式。\n",
    "3. layernorm：包含两个两个层归一化，与GPT一致，均为前层归一化和后层归一化，确保反向传播过程中的数值的稳定性；\n",
    "## norm\n",
    "模型的最后一层层归一化\n",
    "## rotary_emb\n",
    "### 概述\n",
    "Rotary Position Embedding, RoPE模块，是一种位置编码技术，用于在Transformer模型中引入序列中token的相对位置信息，与传统绝对位置编码不同，RoPE通过旋转变换将位置信息融入到Query和Key向量中。\n",
    "### 原理\n",
    "将Query和Key的每一对维度（q2i, q2i+1）视作一个二维坐标；\n",
    "根据位置索引pos和一个频率参数，将这个坐标旋转一个角度；\n",
    "旋转角度θ=pos * base^(-2i/d)，角度随位置线性增长，i越大，角度越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1590f114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Model(\n",
       "  (embed_tokens): Embedding(151936, 896)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x Qwen2DecoderLayer(\n",
       "      (self_attn): Qwen2Attention(\n",
       "        (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "        (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "        (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "      )\n",
       "      (mlp): Qwen2MLP(\n",
       "        (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "        (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检视模型\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa8315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[108386,   3837, 105043, 100165,  11319, 151643],\n",
      "        [ 35946, 100165, 104993,   3837, 104198,  56568],\n",
      "        [102048,  12857,  44928, 151643, 151643, 151643]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([3, 6, 896])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[ -1.1582,  -0.6299,   1.4277,  ...,   6.9180,   0.4622,   1.4111],\n",
       "         [  2.8145,   5.9492,   4.4766,  ...,   2.1406,   3.9102,  -3.5078],\n",
       "         [  5.2148,   6.3672,   4.6055,  ...,   8.7422,   0.4136,  -8.5625],\n",
       "         [ -2.2637,   3.3730,  -1.2910,  ...,   2.7227,   0.6890,  -7.5312],\n",
       "         [  3.9941,   4.8789,   2.0703,  ...,   2.5059,  -3.2148,  -8.0234],\n",
       "         [ -2.3848,  -0.6816,   5.1914,  ...,   4.0547,   2.9180, -15.1484]],\n",
       "\n",
       "        [[  1.8525,   4.4531,   1.5273,  ...,   7.7227,   1.2178,   0.0757],\n",
       "         [ -4.9336,  -1.3486,  -6.9180,  ...,   2.4961,  -2.9961,  11.0469],\n",
       "         [ -0.1310,   3.7656,   0.7329,  ...,   4.9414,   0.6860,  -4.6328],\n",
       "         [  1.7012,  -0.8330,  -1.4219,  ...,   2.9258,   0.3335,   0.7085],\n",
       "         [  0.9697,   0.5176,  -0.3689,  ...,   0.5933,  -0.9019,  -7.4922],\n",
       "         [ -6.7305,  -0.1810,  -0.9126,  ...,   3.7031,  -0.3247,   7.4336]],\n",
       "\n",
       "        [[ -0.9585,  -3.6055,   4.6133,  ...,   0.9863,  -2.8906,   5.3555],\n",
       "         [ -2.9414,  -5.7227,  -3.5938,  ...,  -5.2461,  -3.5195,   5.9102],\n",
       "         [  0.2705,  -3.8340,  -2.3223,  ...,   1.0176,  -1.5527, -12.7969],\n",
       "         [ -4.0273,   1.4355,   0.8306,  ...,   0.1421,  -2.4004,  -9.4766],\n",
       "         [ -0.8462,   1.1367,  -0.7632,  ...,  -0.1255,  -5.1953,  -5.5586],\n",
       "         [  0.0325,   1.8730,  -1.1357,  ...,   2.0332,  -3.6250,  -9.8359]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型输出\n",
    "inputs_on_gpu = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "print(inputs_on_gpu)\n",
    "outputs = model(**inputs_on_gpu) # **表示解包字典为关键字参数形式，传入函数\n",
    "print(outputs.last_hidden_state.shape) # batch_size, sequence_length, hidden_size\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71efaeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationModel(\n",
       "  (base_model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    \"\"\"LLM 二分类任务模型（基于Base Model：Qwen/Qwen2.5-0.5B-Instruct）\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module, hidden_size: int) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            base_model (torch.Module): 基础模型\n",
    "            hidden_size (int): 基础Transformers层hidden size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.net = nn.Sequential(nn.Linear(in_features=hidden_size, out_features=1, dtype=torch.half))\n",
    "        \n",
    "    def forward(self, inputs_ids: torch.Tensor, attention_mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        \"\"\"前向传播\n",
    "\n",
    "        Args:\n",
    "            inputs_ids (torch.Tensor): 输入Token IDS\n",
    "            attention_mask (torch.Tensor, optional): 填充向量表示. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 分类输出\n",
    "        \"\"\"\n",
    "        base_model_outs = self.base_model(inputs_ids, attention_mask)\n",
    "        return self.net(base_model_outs.last_hidden_state)\n",
    "classification_model = ClassificationModel(base_model=model, hidden_size=model.config.hidden_size).to(\"cuda\")\n",
    "classification_model(inputs_ids=inputs_ids)\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397be949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='storage.googleapis.com', port=443): Max retries exceeded with url: /kaggle-competitions-data/kaggle-v2/86518/9809560/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1769826625&Signature=gAC3WlQ3n%2FdYz%2FYRcxTC%2B3%2FShRxfftaiiw2DUO3JHCOvnmfKzCBZ2hzxy%2FIp1MPfBItsfYdYDtf1rTOHCyAnC3yWP1FyKTS58PXWurSLL%2BK4BdYZMHH9OgBTA3x6806JeZpsANqcwyDS3RupZWI434wA7eC8Cy6uB10ALBsvAAnUYroMk6h%2B5URmYAtAKcM9lS%2BVPEnvETPLWV6CaesjLHpWtQYW43%2FPQUvzPp1RXq4EW9wfgkQxVvY0gkkZ%2FGRw5Z1XUt6wlNAOKqBq7gct4%2FZ1RvD8d6EGjR0eCF402Vz6uF7QYpKikxclRWHLVt%2Fo1jYo8Q7pID%2BYdjbYcLKv5w%3D%3D&response-content-disposition=attachment%3B+filename%3Dllm-classification-finetuning.zip (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1147)')))\n"
     ]
    }
   ],
   "source": [
    "# 首先加载Kaggle数据集\n",
    "!kaggle competitions download -c llm-classification-finetuning\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "path = 'llm-classification-finetuning'\n",
    "if not os.path.exists(path=path):\n",
    "    os.makedirs(name=path)\n",
    "    fp = zipfile.ZipFile(file='llm-classification-finetuning.zip', mode='r')\n",
    "    fp.extractall(path)\n",
    "\n",
    "train_csv_data = pd.read_csv(filepath_or_buffer='llm-classification-finetuning/train.csv')\n",
    "test_csv_data = pd.read_csv(filepath_or_buffer='llm-classification-finetuning/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b7d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 特征工程，将数据集划分为text：prompt+response，labels：1/0\n",
    "train_data = []\n",
    "\n",
    "for _, row in train_csv_data.iterrows():\n",
    "    if row['winner_model_a'] == 1:\n",
    "        train_data.append([row['prompt'] + row['response_a'], 1])\n",
    "    else:\n",
    "        train_data.append([row['prompt'] + row['response_b'], 0])\n",
    "        \n",
    "    if row['winner_model_b'] == 1:\n",
    "        train_data.append([row['prompt'] + row['response_b'], 1])\n",
    "    else:\n",
    "        train_data.append([row['prompt'] + row['response_a'], 0])\n",
    "        \n",
    "    if row['winner_tie'] == 1:\n",
    "        train_data.append([row['prompt'] + row['response_a'], 1])\n",
    "        train_data.append([row['prompt'] + row['response_b'], 1])\n",
    "\n",
    "all_data = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n",
    "# train_data.to_csv(\"llm-classification-finetuning/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b35335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train、test、val dataset is 105333, 30095, 15048\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集7:2:1（训练、测试、验证）\n",
    "train_end_idx = int(len(all_data) * 0.7)\n",
    "test_end_idx = train_end_idx + int(len(all_data) * 0.2)\n",
    "train_data = all_data[:train_end_idx]\n",
    "test_data = all_data[train_end_idx:test_end_idx]\n",
    "val_data = all_data[test_end_idx:]\n",
    "\n",
    "print(f\"Size of train、test、val dataset is {len(train_data)}, {len(test_data)}, {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8caf00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据集\n",
    "train_data.to_csv(\"llm-classification-finetuning/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"llm-classification-finetuning/test_data.csv\", index=False)\n",
    "val_data.to_csv(\"llm-classification-finetuning/val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "506e5c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105328</th>\n",
       "      <td>[\"What did Bilbo have in his pocket?\",\"When di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105329</th>\n",
       "      <td>[\"What did Bilbo have in his pocket?\",\"When di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105330</th>\n",
       "      <td>[\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105331</th>\n",
       "      <td>[\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105332</th>\n",
       "      <td>[\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105333 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       [\"Is it morally right to try to have a certain...      1\n",
       "1       [\"Is it morally right to try to have a certain...      0\n",
       "2       [\"What is the difference between marriage lice...      0\n",
       "3       [\"What is the difference between marriage lice...      1\n",
       "4       [\"explain function calling. how would you call...      0\n",
       "...                                                   ...    ...\n",
       "105328  [\"What did Bilbo have in his pocket?\",\"When di...      0\n",
       "105329  [\"What did Bilbo have in his pocket?\",\"When di...      1\n",
       "105330  [\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...      0\n",
       "105331  [\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...      0\n",
       "105332  [\"Reescreva as fun\\u00e7\\u00f5es, agrupando-as...      1\n",
       "\n",
       "[105333 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6918e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc5bfdd534b42e3baff85b2e3861fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8817da5e69e4f6badc9f09faa375f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e88dda46ae4ad6ba497eb3599d89d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datasets库（兼容处理Kaggle数据集）\n",
    "from datasets import load_dataset\n",
    "# 读取数据集（并且可以同时加载多个数据集，并且对不同数据集划分指定文件）\n",
    "# train_dataset = load_dataset(\"csv\", data_files=\"llm-classification-finetuning/train.csv\")\n",
    "# all_dataset = load_dataset(\"csv\", data_files=[\"llm-classification-finetuning/train.csv\", \"llm-classification-finetuning/test.csv\"])\n",
    "all_dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"llm-classification-finetuning/train_data.csv\",\n",
    "    \"test\" : \"llm-classification-finetuning/test_data.csv\",\n",
    "    \"val\"  : \"llm-classification-finetuning/val_data.csv\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe00bc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"][\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\\\n\\\\nHere are some arguments in favor of and against such policies:\\\\n\\\\n**Arguments in favor:**\\\\n\\\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\\\n\\\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\\\n\\\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\\\n\\\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\\\n\\\\n**Arguments against:**\\\\n\\\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\\\n\\\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\\\n\\\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\\\"tokens,\\\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\\\n\\\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\\\n\\\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\\\"color-blind\\\\\" or \\\\\"gender-blind\\\\\" approach to hiring and promotions.\\\\n\\\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\\\n\\\\nLet\\'s slice into the debate with a zest of fun:\\\\n\\\\n**Team Pineapple:** \\\\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It\\'s the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\\\"\\\\n\\\\n**Team No-Pineapple:** \\\\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple\\'s sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\\\"\\\\n\\\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it\\'s a match made in heaven, others say it\\'s a culinary crime. But let\\'s face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there\\'s room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\\\u00e9tit, or as they say in pineapple paradise, \\\\\"Aloha-ppetite!\\\\\" \\\\ud83c\\\\udf4d\\\\ud83c\\\\udf55\"]',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50a61ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05f0fad6cf14ce69933cd6461d4160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af64181f50244597a83439db328801fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b6f5c5351148d6be2ac116f606cba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing_extensions import Any\n",
    "\n",
    "def tokenize_function(dataset: dict[str, Any]):\n",
    "    \"\"\"对每个输入进行tokenize操作\n",
    "\n",
    "    Args:\n",
    "        dataset (dict[str, Any]): 数据集\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: 数据集Dict\n",
    "    \"\"\"\n",
    "    return tokenizer(dataset['text'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_datasets = all_dataset.map(tokenize_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5926d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [1183,\n",
       "  3872,\n",
       "  432,\n",
       "  56223,\n",
       "  1290,\n",
       "  311,\n",
       "  1430,\n",
       "  311,\n",
       "  614,\n",
       "  264,\n",
       "  3654,\n",
       "  11414,\n",
       "  315,\n",
       "  27485,\n",
       "  389,\n",
       "  90150,\n",
       "  9892,\n",
       "  30,\n",
       "  2198,\n",
       "  3925,\n",
       "  11,\n",
       "  1558,\n",
       "  77082,\n",
       "  9173,\n",
       "  389,\n",
       "  264,\n",
       "  22502,\n",
       "  30,\n",
       "  67585,\n",
       "  323,\n",
       "  2968,\n",
       "  752,\n",
       "  2464,\n",
       "  4226,\n",
       "  1189,\n",
       "  9868,\n",
       "  785,\n",
       "  3405,\n",
       "  315,\n",
       "  3425,\n",
       "  432,\n",
       "  374,\n",
       "  56223,\n",
       "  1290,\n",
       "  311,\n",
       "  9213,\n",
       "  369,\n",
       "  264,\n",
       "  3654,\n",
       "  11414,\n",
       "  315,\n",
       "  27485,\n",
       "  304,\n",
       "  90150,\n",
       "  9892,\n",
       "  374,\n",
       "  264,\n",
       "  6351,\n",
       "  30208,\n",
       "  4265,\n",
       "  429,\n",
       "  17601,\n",
       "  37764,\n",
       "  315,\n",
       "  50741,\n",
       "  11,\n",
       "  21777,\n",
       "  11,\n",
       "  19492,\n",
       "  11,\n",
       "  323,\n",
       "  21240,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  8420,\n",
       "  525,\n",
       "  1045,\n",
       "  5977,\n",
       "  304,\n",
       "  4694,\n",
       "  315,\n",
       "  323,\n",
       "  2348,\n",
       "  1741,\n",
       "  10186,\n",
       "  7190,\n",
       "  77,\n",
       "  1699,\n",
       "  334,\n",
       "  19139,\n",
       "  304,\n",
       "  4694,\n",
       "  66963,\n",
       "  59,\n",
       "  77,\n",
       "  1699,\n",
       "  16,\n",
       "  13,\n",
       "  3070,\n",
       "  33092,\n",
       "  287,\n",
       "  40043,\n",
       "  758,\n",
       "  25310,\n",
       "  1361,\n",
       "  66963,\n",
       "  10973,\n",
       "  614,\n",
       "  34801,\n",
       "  1012,\n",
       "  1212,\n",
       "  52759,\n",
       "  304,\n",
       "  11438,\n",
       "  12783,\n",
       "  4152,\n",
       "  311,\n",
       "  5257,\n",
       "  12752,\n",
       "  11,\n",
       "  32132,\n",
       "  11,\n",
       "  323,\n",
       "  3590,\n",
       "  29640,\n",
       "  13,\n",
       "  362,\n",
       "  59076,\n",
       "  369,\n",
       "  264,\n",
       "  3151,\n",
       "  11414,\n",
       "  646,\n",
       "  387,\n",
       "  3884,\n",
       "  438,\n",
       "  264,\n",
       "  90752,\n",
       "  6629,\n",
       "  311,\n",
       "  2621,\n",
       "  3267,\n",
       "  323,\n",
       "  14195,\n",
       "  21240,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  17,\n",
       "  13,\n",
       "  3070,\n",
       "  35186,\n",
       "  11519,\n",
       "  64971,\n",
       "  66963,\n",
       "  422,\n",
       "  8034,\n",
       "  11438,\n",
       "  7263,\n",
       "  646,\n",
       "  18379,\n",
       "  5480,\n",
       "  27746,\n",
       "  323,\n",
       "  4009,\n",
       "  264,\n",
       "  26829,\n",
       "  2088,\n",
       "  315,\n",
       "  38455,\n",
       "  13,\n",
       "  1096,\n",
       "  646,\n",
       "  2990,\n",
       "  311,\n",
       "  2664,\n",
       "  19554,\n",
       "  369,\n",
       "  11104,\n",
       "  323,\n",
       "  8232,\n",
       "  438,\n",
       "  264,\n",
       "  4361,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  18,\n",
       "  13,\n",
       "  3070,\n",
       "  50745,\n",
       "  315,\n",
       "  47279,\n",
       "  66963,\n",
       "  20037,\n",
       "  11550,\n",
       "  369,\n",
       "  8778,\n",
       "  13042,\n",
       "  304,\n",
       "  6240,\n",
       "  646,\n",
       "  1492,\n",
       "  5978,\n",
       "  429,\n",
       "  3198,\n",
       "  614,\n",
       "  6144,\n",
       "  10488,\n",
       "  311,\n",
       "  11912,\n",
       "  304,\n",
       "  862,\n",
       "  30033,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  19,\n",
       "  13,\n",
       "  3070,\n",
       "  9030,\n",
       "  76249,\n",
       "  66963,\n",
       "  61597,\n",
       "  23160,\n",
       "  315,\n",
       "  8778,\n",
       "  6036,\n",
       "  646,\n",
       "  30640,\n",
       "  323,\n",
       "  14907,\n",
       "  1008,\n",
       "  3198,\n",
       "  323,\n",
       "  7571,\n",
       "  311,\n",
       "  22729,\n",
       "  11438,\n",
       "  12783,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  334,\n",
       "  19139,\n",
       "  2348,\n",
       "  66963,\n",
       "  59,\n",
       "  77,\n",
       "  1699,\n",
       "  16,\n",
       "  13,\n",
       "  3070,\n",
       "  45695,\n",
       "  77300,\n",
       "  2554,\n",
       "  66963,\n",
       "  20037,\n",
       "  264,\n",
       "  42042,\n",
       "  369,\n",
       "  8778,\n",
       "  13042,\n",
       "  2578,\n",
       "  2990,\n",
       "  311,\n",
       "  279,\n",
       "  20431,\n",
       "  476,\n",
       "  8729,\n",
       "  315,\n",
       "  9931,\n",
       "  21240,\n",
       "  11,\n",
       "  1380,\n",
       "  2953,\n",
       "  2578,\n",
       "  387,\n",
       "  44436,\n",
       "  369,\n",
       "  9892,\n",
       "  8818,\n",
       "  1660,\n",
       "  14988,\n",
       "  11,\n",
       "  4936,\n",
       "  311,\n",
       "  3367,\n",
       "  279,\n",
       "  9825,\n",
       "  2169,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  17,\n",
       "  13,\n",
       "  3070,\n",
       "  63067,\n",
       "  45260,\n",
       "  51247,\n",
       "  82,\n",
       "  66963,\n",
       "  76413,\n",
       "  17585,\n",
       "  429,\n",
       "  14402,\n",
       "  323,\n",
       "  20249,\n",
       "  1265,\n",
       "  387,\n",
       "  3118,\n",
       "  21063,\n",
       "  389,\n",
       "  37459,\n",
       "  323,\n",
       "  42684,\n",
       "  11,\n",
       "  537,\n",
       "  9825,\n",
       "  13,\n",
       "  2379,\n",
       "  8679,\n",
       "  429,\n",
       "  84818,\n",
       "  1410,\n",
       "  29385,\n",
       "  279,\n",
       "  4271,\n",
       "  315,\n",
       "  11438,\n",
       "  421,\n",
       "  2686,\n",
       "  14988,\n",
       "  7775,\n",
       "  525,\n",
       "  28926,\n",
       "  311,\n",
       "  3367,\n",
       "  9825,\n",
       "  11550,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  18,\n",
       "  13,\n",
       "  3070,\n",
       "  3323,\n",
       "  2142,\n",
       "  66963,\n",
       "  2619,\n",
       "  374,\n",
       "  264,\n",
       "  5214,\n",
       "  429,\n",
       "  3198,\n",
       "  20822,\n",
       "  311,\n",
       "  3367,\n",
       "  84818,\n",
       "  2578,\n",
       "  387,\n",
       "  3884,\n",
       "  438,\n",
       "  7245,\n",
       "  30566,\n",
       "  52318,\n",
       "  77369,\n",
       "  862,\n",
       "  55908,\n",
       "  323,\n",
       "  13581,\n",
       "  6388,\n",
       "  311,\n",
       "  70554,\n",
       "  4221,\n",
       "  17639,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  19,\n",
       "  13,\n",
       "  3070,\n",
       "  46,\n",
       "  3004,\n",
       "  6383,\n",
       "  2404,\n",
       "  315,\n",
       "  64971,\n",
       "  66963,\n",
       "  434,\n",
       "  86495,\n",
       "  1172,\n",
       "  389,\n",
       "  9825,\n",
       "  2578,\n",
       "  30705,\n",
       "  1008,\n",
       "  2989,\n",
       "  13566,\n",
       "  315,\n",
       "  19492,\n",
       "  11,\n",
       "  1741,\n",
       "  438,\n",
       "  6957,\n",
       "  11,\n",
       "  56878,\n",
       "  11,\n",
       "  79331,\n",
       "  4004,\n",
       "  11,\n",
       "  476,\n",
       "  7244,\n",
       "  16725,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  66243,\n",
       "  11,\n",
       "  279,\n",
       "  44551,\n",
       "  315,\n",
       "  37078,\n",
       "  369,\n",
       "  264,\n",
       "  3654,\n",
       "  11414,\n",
       "  315,\n",
       "  27485,\n",
       "  304,\n",
       "  90150,\n",
       "  9892,\n",
       "  13798,\n",
       "  389,\n",
       "  825,\n",
       "  594,\n",
       "  30208,\n",
       "  12626,\n",
       "  323,\n",
       "  279,\n",
       "  2266,\n",
       "  304,\n",
       "  892,\n",
       "  1741,\n",
       "  10186,\n",
       "  525,\n",
       "  11537,\n",
       "  13,\n",
       "  1298,\n",
       "  2700,\n",
       "  315,\n",
       "  9825,\n",
       "  19492,\n",
       "  11550,\n",
       "  3545,\n",
       "  17585,\n",
       "  429,\n",
       "  1493,\n",
       "  10953,\n",
       "  525,\n",
       "  5871,\n",
       "  438,\n",
       "  264,\n",
       "  65643,\n",
       "  16953,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2188,\n",
       "  5619,\n",
       "  2070,\n",
       "  11,\n",
       "  1393,\n",
       "  19386,\n",
       "  1231,\n",
       "  17585,\n",
       "  369,\n",
       "  264,\n",
       "  7245,\n",
       "  3423,\n",
       "  94349,\n",
       "  2105,\n",
       "  476,\n",
       "  7245,\n",
       "  12968,\n",
       "  94349,\n",
       "  2105,\n",
       "  5486,\n",
       "  311,\n",
       "  23134,\n",
       "  323,\n",
       "  35971,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  23227,\n",
       "  8040,\n",
       "  323,\n",
       "  33675,\n",
       "  1969,\n",
       "  15516,\n",
       "  2908,\n",
       "  279,\n",
       "  24154,\n",
       "  315,\n",
       "  1493,\n",
       "  14830,\n",
       "  323,\n",
       "  36006,\n",
       "  311,\n",
       "  4211,\n",
       "  10186,\n",
       "  429,\n",
       "  11926,\n",
       "  50741,\n",
       "  323,\n",
       "  21777,\n",
       "  1393,\n",
       "  76291,\n",
       "  8225,\n",
       "  15917,\n",
       "  13,\n",
       "  1084,\n",
       "  374,\n",
       "  1083,\n",
       "  2989,\n",
       "  311,\n",
       "  2621,\n",
       "  279,\n",
       "  3704,\n",
       "  11137,\n",
       "  315,\n",
       "  9825,\n",
       "  31205,\n",
       "  11,\n",
       "  1741,\n",
       "  438,\n",
       "  39611,\n",
       "  15470,\n",
       "  11,\n",
       "  975,\n",
       "  25843,\n",
       "  8172,\n",
       "  11513,\n",
       "  11,\n",
       "  323,\n",
       "  6853,\n",
       "  315,\n",
       "  32345,\n",
       "  10186,\n",
       "  11,\n",
       "  311,\n",
       "  1855,\n",
       "  458,\n",
       "  4573,\n",
       "  1380,\n",
       "  5019,\n",
       "  702,\n",
       "  279,\n",
       "  6638,\n",
       "  311,\n",
       "  11996,\n",
       "  3118,\n",
       "  389,\n",
       "  862,\n",
       "  16928,\n",
       "  323,\n",
       "  19026,\n",
       "  47891,\n",
       "  24765,\n",
       "  11,\n",
       "  279,\n",
       "  4231,\n",
       "  6284,\n",
       "  57341,\n",
       "  390,\n",
       "  1241,\n",
       "  10721,\n",
       "  429,\n",
       "  702,\n",
       "  17779,\n",
       "  16675,\n",
       "  323,\n",
       "  13856,\n",
       "  12632,\n",
       "  25,\n",
       "  1558,\n",
       "  77082,\n",
       "  9173,\n",
       "  389,\n",
       "  264,\n",
       "  22502,\n",
       "  30,\n",
       "  576,\n",
       "  34048,\n",
       "  26646,\n",
       "  315,\n",
       "  77082,\n",
       "  389,\n",
       "  22502,\n",
       "  11,\n",
       "  3881,\n",
       "  438,\n",
       "  58003,\n",
       "  22502,\n",
       "  11,\n",
       "  374,\n",
       "  264,\n",
       "  4017,\n",
       "  398,\n",
       "  58574,\n",
       "  8544,\n",
       "  1380,\n",
       "  12656,\n",
       "  67443,\n",
       "  8049,\n",
       "  323,\n",
       "  22502,\n",
       "  3999,\n",
       "  1671,\n",
       "  8665,\n",
       "  7110,\n",
       "  77,\n",
       "  1699,\n",
       "  10061,\n",
       "  594,\n",
       "  15983,\n",
       "  1119,\n",
       "  279,\n",
       "  11004,\n",
       "  448,\n",
       "  264,\n",
       "  80531,\n",
       "  315,\n",
       "  2464,\n",
       "  7190,\n",
       "  77,\n",
       "  1699,\n",
       "  334,\n",
       "  14597,\n",
       "  41509,\n",
       "  22377,\n",
       "  66963,\n",
       "  7245,\n",
       "  77045,\n",
       "  11,\n",
       "  9834,\n",
       "  0,\n",
       "  41509,\n",
       "  22377,\n",
       "  389,\n",
       "  22502,\n",
       "  374,\n",
       "  1075,\n",
       "  264,\n",
       "  11321,\n",
       "  4614,\n",
       "  304,\n",
       "  697,\n",
       "  10780,\n",
       "  13,\n",
       "  576,\n",
       "  10226,\n",
       "  10267,\n",
       "  292,\n",
       "  1880,\n",
       "  315,\n",
       "  77082,\n",
       "  26757,\n",
       "  12799,\n",
       "  1206,\n",
       "  287,\n",
       "  448,\n",
       "  93660,\n",
       "  13515,\n",
       "  476,\n",
       "  40352,\n",
       "  11450,\n",
       "  264,\n",
       "  17172,\n",
       "  12060,\n",
       "  429,\n",
       "  1729,\n",
       "  3848,\n",
       "  3941,\n",
       "  279,\n",
       "  86747,\n",
       "  17951,\n",
       "  315,\n",
       "  17923,\n",
       "  2090,\n",
       "  13,\n",
       "  1084,\n",
       "  594,\n",
       "  279,\n",
       "  386,\n",
       "  36389,\n",
       "  2825,\n",
       "  300,\n",
       "  315,\n",
       "  87770,\n",
       "  11,\n",
       "  1380,\n",
       "  10226,\n",
       "  20027,\n",
       "  93660,\n",
       "  304,\n",
       "  264,\n",
       "  96343,\n",
       "  321,\n",
       "  517,\n",
       "  502,\n",
       "  2969,\n",
       "  460,\n",
       "  68,\n",
       "  0,\n",
       "  22245,\n",
       "  77,\n",
       "  1699,\n",
       "  334,\n",
       "  14597,\n",
       "  2308,\n",
       "  9299,\n",
       "  482,\n",
       "  22377,\n",
       "  66963,\n",
       "  7245,\n",
       "  2753,\n",
       "  1616,\n",
       "  11,\n",
       "  537,\n",
       "  304,\n",
       "  264,\n",
       "  3526,\n",
       "  87770,\n",
       "  0,\n",
       "  41509,\n",
       "  22377,\n",
       "  374,\n",
       "  264,\n",
       "  13779,\n",
       "  429,\n",
       "  1265,\n",
       "  387,\n",
       "  7015,\n",
       "  65,\n",
       "  43561,\n",
       "  389,\n",
       "  264,\n",
       "  34048,\n",
       "  13779,\n",
       "  625,\n",
       "  1650,\n",
       "  11,\n",
       "  537,\n",
       "  3457,\n",
       "  20172,\n",
       "  2228,\n",
       "  389,\n",
       "  264,\n",
       "  31342,\n",
       "  15983,\n",
       "  315,\n",
       "  22502,\n",
       "  13,\n",
       "  576,\n",
       "  3381,\n",
       "  315,\n",
       "  8205,\n",
       "  11,\n",
       "  10581,\n",
       "  1881,\n",
       "  17163,\n",
       "  21513,\n",
       "  287,\n",
       "  448,\n",
       "  77082,\n",
       "  594,\n",
       "  30605,\n",
       "  658,\n",
       "  93946,\n",
       "  374,\n",
       "  3322,\n",
       "  311,\n",
       "  1281,\n",
       "  14811,\n",
       "  37518,\n",
       "  2484,\n",
       "  304,\n",
       "  862,\n",
       "  41020,\n",
       "  28660,\n",
       "  13,\n",
       "  13655,\n",
       "  279,\n",
       "  77082,\n",
       "  304,\n",
       "  279,\n",
       "  13779,\n",
       "  32466,\n",
       "  1380,\n",
       "  432,\n",
       "  17180,\n",
       "  0,\n",
       "  22245,\n",
       "  77,\n",
       "  1699,\n",
       "  1655,\n",
       "  279,\n",
       "  835,\n",
       "  315,\n",
       "  279,\n",
       "  1899,\n",
       "  11,\n",
       "  3425,\n",
       "  77082,\n",
       "  17180,\n",
       "  389,\n",
       "  22502,\n",
       "  374,\n",
       "  264,\n",
       "  4925,\n",
       "  315,\n",
       "  4345,\n",
       "  12656,\n",
       "  13,\n",
       "  4329,\n",
       "  1977,\n",
       "  432,\n",
       "  594,\n",
       "  264,\n",
       "  2432,\n",
       "  1865,\n",
       "  304,\n",
       "  22274,\n",
       "  11,\n",
       "  3800,\n",
       "  1977,\n",
       "  432,\n",
       "  594,\n",
       "  264,\n",
       "  57341,\n",
       "  9778,\n",
       "  13,\n",
       "  1988,\n",
       "  1077,\n",
       "  594,\n",
       "  3579,\n",
       "  432,\n",
       "  11,\n",
       "  304,\n",
       "  279,\n",
       "  1879,\n",
       "  315,\n",
       "  22502,\n",
       "  89671,\n",
       "  11,\n",
       "  1380,\n",
       "  498,\n",
       "  646,\n",
       "  1477,\n",
       "  4297,\n",
       "  504,\n",
       "  33230,\n",
       "  12546,\n",
       "  311,\n",
       "  1147,\n",
       "  85570,\n",
       "  11,\n",
       "  1052,\n",
       "  594,\n",
       "  3054,\n",
       "  369,\n",
       "  264,\n",
       "  2632,\n",
       "  77082,\n",
       "  281,\n",
       "  8759,\n",
       "  9802,\n",
       "  13,\n",
       "  2055,\n",
       "  11,\n",
       "  421,\n",
       "  697,\n",
       "  12656,\n",
       "  67443,\n",
       "  80313,\n",
       "  429,\n",
       "  10226,\n",
       "  323,\n",
       "  21878,\n",
       "  88,\n",
       "  26646,\n",
       "  11,\n",
       "  728,\n",
       "  8305,\n",
       "  323,\n",
       "  1077,\n",
       "  697,\n",
       "  77082,\n",
       "  5181,\n",
       "  11466,\n",
       "  45988,\n",
       "  429,\n",
       "  43208,\n",
       "  17163,\n",
       "  83308,\n",
       "  30352,\n",
       "  13,\n",
       "  13481,\n",
       "  906,\n",
       "  3770,\n",
       "  15,\n",
       "  15,\n",
       "  68,\n",
       "  24,\n",
       "  16903,\n",
       "  11,\n",
       "  476,\n",
       "  438,\n",
       "  807,\n",
       "  1977,\n",
       "  304,\n",
       "  77082,\n",
       "  49752,\n",
       "  11,\n",
       "  7245,\n",
       "  32,\n",
       "  385,\n",
       "  4223,\n",
       "  12,\n",
       "  602,\n",
       "  295,\n",
       "  632,\n",
       "  0,\n",
       "  2105,\n",
       "  1124,\n",
       "  661,\n",
       "  23,\n",
       "  18,\n",
       "  66,\n",
       "  59,\n",
       "  661,\n",
       "  69,\n",
       "  19,\n",
       "  67,\n",
       "  59,\n",
       "  661,\n",
       "  23,\n",
       "  18,\n",
       "  66,\n",
       "  59,\n",
       "  661,\n",
       "  69,\n",
       "  20,\n",
       "  20,\n",
       "  1341,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  151643,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11c0bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "train_dataloader = DataLoader(dataset=tokenized_datasets['train'], batch_size=500, shuffle=True, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(dataset=tokenized_datasets['test'], batch_size=500, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(dataset=tokenized_datasets['val'], batch_size=500, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b14cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  1183,  36337,    311,  ..., 151643, 151643, 151643],\n",
      "        [  1183,    531,    438,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724ccad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoModelForSequenceClassification\n",
    "# 模型保存路径\n",
    "model_dir = \"/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# 初始化一个TrainingArguments对象，用于存储和管理训练参数配置（实例构造函数参数说明）\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_dir}/trainer\",\n",
    "    logging_dir=f\"{model_dir}/trainer/runs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a5e5d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForSequenceClassification(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=896, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# 对于分类任务，亦可通过AutoModelForSequenceClassification指定num_labels（分类头数量）来进行分类任务\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=checkpoint, num_labels=1).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e117701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49602/1722634407.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 创建Trainer\n",
    "from transformers import Trainer\n",
    "# 初始化TrainingArguments对象，传入一个参数“test-trainer”作为输出目录的基础名称\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-trainer\",        # 检查点保存目录\n",
    "    save_strategy=\"epoch\",            # 保存策略（每次epoch后保存）\n",
    "    save_total_limit=3,               # 仅保存最新3个检查点（自动删除旧检查点）\n",
    "    load_best_model_at_end=True,      # 训练结束自动加载验证集最佳模型\n",
    "    metric_for_best_model=\"accuracy\", # 选择最佳模型的指标\n",
    "    greater_is_better=True,           # 指标越大越好（如：准确率）False表示越小越好，用于loss\n",
    "    num_train_epochs=3                # 设置迭代次数\n",
    ")\n",
    "# 冻结模型Transformers层\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 解冻分类层\n",
    "for param in model.score.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1fbb09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 调用Trainer对象的train()方法启动模型的训练过程\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/accelerate/accelerator.py:2734\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/miniconda/envs/d2l/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "# 调用Trainer对象的train()方法启动模型的训练过程，设置自动加载output_dir中最新检查点\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
