{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d9ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入AutoModel类，该类允许自动从预训练模型库加载模型\n",
    "from modelscope import AutoModel, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "checkpoint = \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34071743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/openai-community/gpt2\n",
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/openai-community/gpt2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayerNorm((768,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载GPT-2模型 & 分词器\n",
    "model: nn.Module = AutoModel.from_pretrained(pretrained_model_name_or_path=checkpoint, num_labels=1).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d195d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b2c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Classification(\n",
       "  (net): Sequential(\n",
       "    (0): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义模型分类层\n",
    "class GPT2Classification(nn.Module):\n",
    "    \"\"\"基于GPT2的分类层\n",
    "\n",
    "    Args:\n",
    "        nn (Module): PyTorch Module\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: nn.Module, label_num: int):\n",
    "        super().__init__()\n",
    "        # 单层label_num结构\n",
    "        self.net = nn.Sequential(pretrained_model, nn.Linear(in_features=model.ln_f.weight.shape[0], out_features=label_num))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前向传播\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 输入张量\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 分类结果\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "classification_model = GPT2Classification(pretrained_model=model, label_num=2)\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65b730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "all_dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"llm-classification-finetuning/train_data.csv\",\n",
    "    \"test\" : \"llm-classification-finetuning/test_data.csv\",\n",
    "    \"val\"  : \"llm-classification-finetuning/val_data.csv\"\n",
    "}) # 从CSV中加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "from transformers import TrainingArguments\n",
    "model_dir = \"/root/.cache/modelscope/hub/models/openai-community/gpt2\" # 模型目录\n",
    "checkpoint_output_dir = \"gpt2-finetuning-checkpoints\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=checkpoint_output_dir, # 检查点保存目录\n",
    "    save_strategy=\"epoch\",            # checkpoint保存策略（每次迭代保存）\n",
    "    save_total_limit=3,               # 最多保存3个检查点，多余检查点删除\n",
    "    load_best_model_at_end=True,      # 训练结束后自动加载验证集最优模型\n",
    "    metric_for_best_model=\"accuracy\", # 选择模型最佳指标\n",
    "    greater_is_better=True,           # True：最佳指标越大越好、False：最佳指标越小越好\n",
    "    num_train_epochs=3,               # 迭代次数\n",
    "    learning_rate=2e-5,               # 学习率\n",
    "    weight_decay=0.01                 # 权重衰减（L2正则化：在训练过程中对模型权重施加惩罚）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 冻结策略（逐步解冻）\n",
    "### 1.先冻结大部分层，仅解冻分类层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
